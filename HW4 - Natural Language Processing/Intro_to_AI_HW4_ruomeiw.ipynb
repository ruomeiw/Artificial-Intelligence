{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d39a2b1",
   "metadata": {},
   "source": [
    "## 95-891 Introduction to AI  \n",
    "## Homework 4: Natural Language Processing - Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "c2932f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ruome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ruome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from itertools import compress\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from adjustText import adjust_text\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109e5a8",
   "metadata": {},
   "source": [
    "## 1. Data ETL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0f05d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         context                                          utterance\n",
      "0      terrified  Job interviews always make me sweat bullets_co...\n",
      "1      terrified                Don't be nervous. Just be prepared.\n",
      "2      terrified  I feel like getting prepared and then having a...\n",
      "3      terrified            Yes but if you stay calm it will be ok.\n",
      "4      terrified         It's hard to stay clam. How do you do it? \n",
      "...          ...                                                ...\n",
      "11346    jealous  Yeah_comma_ I can understand that. Sometimes w...\n",
      "11347  terrified  During this past spring during a bad storm_com...\n",
      "11348  terrified           Oh my gosh!  That had to be super scary!\n",
      "11349  terrified  It sounded like a train going by the house fro...\n",
      "11350  terrified  Yeah_comma_ thank god you are safe.  I don't k...\n",
      "\n",
      "[11351 rows x 2 columns]\n",
      "        context                                          utterance\n",
      "0           sad  I'm so sad because i've read an article about ...\n",
      "1           sad  Ugh_comma_ those articles always get me too......\n",
      "2           sad  she was born premature at home_comma_ she had ...\n",
      "3           sad      Jeez! Its so unfortunate... very sad really. \n",
      "4           sad  yes! And i do believe in God and prayers but g...\n",
      "...         ...                                                ...\n",
      "1372  terrified  Hahaha.. that's kind of goofy in a way. Yeah_c...\n",
      "1373     joyful  Christmas was the best time of year back in th...\n",
      "1374     joyful  I still think it is_comma_ i always get so exc...\n",
      "1375     joyful  Yeah_comma_ it's like there's nothing to worry...\n",
      "1376     joyful   yep thats one of the things that make it so nice\n",
      "\n",
      "[1377 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Importing the datasets\n",
    "fields = ['utterance', 'context']\n",
    "train = pd.read_csv('train.csv', usecols=fields)\n",
    "valid = pd.read_csv('valid.csv', usecols=fields)\n",
    "test = pd.read_csv('test.csv', usecols=fields)\n",
    "\n",
    "# Concat the training data\n",
    "# Filter out the four target sentiments for both the training and testing data\n",
    "sentiments = ['sad', 'jealous', 'joyful', 'terrified']\n",
    "train_data = pd.concat([train, valid])\n",
    "train_data = train_data.loc[train_data['context'].apply(lambda x: x in sentiments)]\n",
    "test_data = test.loc[test['context'].apply(lambda x: x in sentiments)]\n",
    "\n",
    "# Clean the index\n",
    "train_data.index = pd.RangeIndex(len(train_data.index))\n",
    "test_data.index = pd.RangeIndex(len(test_data.index))\n",
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "e3992267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data cleaned: \n",
      "0        Job interviews always make me sweat bullets_co...\n",
      "1                      Don t be nervous  Just be prepared \n",
      "2        I feel like getting prepared and then having a...\n",
      "3                  Yes but if you stay calm it will be ok \n",
      "4               It s hard to stay clam  How do you do it  \n",
      "                               ...                        \n",
      "11346    Yeah_comma_ I can understand that  Sometimes w...\n",
      "11347    During this past spring during a bad storm_com...\n",
      "11348             Oh my gosh   That had to be super scary \n",
      "11349    It sounded like a train going by the house fro...\n",
      "11350    Yeah_comma_ thank god you are safe   I don t k...\n",
      "Name: utterance, Length: 11351, dtype: object\n",
      "------------------------------------------------------\n",
      "Test Data cleaned: \n",
      "0       I m so sad because i ve read an article about ...\n",
      "1       Ugh_comma_ those articles always get me too   ...\n",
      "2       she was born premature at home_comma_ she had ...\n",
      "3           Jeez  Its so unfortunate    very sad really  \n",
      "4       yes  And i do believe in God and prayers but g...\n",
      "                              ...                        \n",
      "1372    Hahaha   that s kind of goofy in a way  Yeah_c...\n",
      "1373    Christmas was the best time of year back in th...\n",
      "1374    I still think it is_comma_ i always get so exc...\n",
      "1375    Yeah_comma_ it s like there s nothing to worry...\n",
      "1376     yep thats one of the things that make it so nice\n",
      "Name: utterance, Length: 1377, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruome\\AppData\\Local\\Temp\\ipykernel_828\\2059654676.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['utterance'] = test_data['utterance'].apply(lambda row: re.sub(r'\\W', ' ', str(row)))\n"
     ]
    }
   ],
   "source": [
    "# Synthesize training attributes and labels\n",
    "# Getting the train labels\n",
    "train_labels_unique = list(train_data['context'].unique())\n",
    "label_mapper = {}\n",
    "num = 0\n",
    "for label in train_labels_unique:\n",
    "    label_mapper[label] = num\n",
    "    num += 1\n",
    "\n",
    "train_labels = list(train_data['context'])\n",
    "train_labels_encoded = []\n",
    "for label in train_labels:\n",
    "    train_labels_encoded.append(label_mapper[label])\n",
    "\n",
    "\n",
    "# Getting test labels\n",
    "labels_test = list(test_data['context'])\n",
    "test_labels_encoded = []\n",
    "for label in labels_test:\n",
    "    test_labels_encoded.append(label_mapper[label])\n",
    "test_labels_encoded = np.array(test_labels_encoded)\n",
    "\n",
    "# Data preprocessing\n",
    "# Remove punctuations and replace \"_comma_\" from the sentence\n",
    "train_data['utterance'] = train_data['utterance'].apply(lambda row: re.sub(r'\\W', ' ', str(row)))\n",
    "\n",
    "test_data['utterance'] = test_data['utterance'].apply(lambda row: re.sub(r'\\W', ' ', str(row)))\n",
    "# test_data['utterance'] = test_data['utterance'].apply(lambda row: row.replace('_comma_', ''))\n",
    "\n",
    "train_data_list_cleaned = train_data['utterance']\n",
    "test_data_list_cleaned = test_data['utterance']\n",
    "\n",
    "print(\"Train Data cleaned: \")\n",
    "print(train_data_list_cleaned)\n",
    "\n",
    "print(\"------------------------------------------------------\")\n",
    "print(\"Test Data cleaned: \")\n",
    "print(test_data_list_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ee173",
   "metadata": {},
   "source": [
    "## 2. Converting the utterances into a sparse bag-of-words (BOW) representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "17501836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of non-zero cells is: 136631\n",
      "The size of encoding array is: 108299891\n",
      "The percentage of words present is: 0.12615986843421662%\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn\n",
    "train_count_vectorizer = CountVectorizer()\n",
    "X = train_count_vectorizer.fit_transform(train_data_list_cleaned)\n",
    "encoding = X.toarray()\n",
    "\n",
    "# Converting counts to binary result\n",
    "present_count = 0\n",
    "for arr in encoding:\n",
    "    arr_present_count = (arr > 0).sum()\n",
    "    present_count += arr_present_count\n",
    "\n",
    "print(\"The count of non-zero cells is: {0}\".format(present_count))\n",
    "print(\"The size of encoding array is: {0}\".format(encoding.size))\n",
    "print(\"The percentage of words present is: {0}%\".format(present_count / encoding.size * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69804883",
   "metadata": {},
   "source": [
    "## 3. Shortcoming of the previous representation of utterance features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abb9bae",
   "metadata": {},
   "source": [
    "In my view, the shortcomings of the BOW representation of utterace features are:\n",
    "\n",
    "1. There many meaningless words counted in the BOW matrix. For example, stopwords like \"the\", \"is\", \"and\", and words like \"_comma_\", which does not help much in the process of training the classifier.\n",
    "\n",
    "2. The percentage of non-zero cells in the entire encoding array is only 0.12615986843421662%. This indicates the matrix is too sparse and we should avoid this.\n",
    "\n",
    "3. No grammar of the utterances, order of words, or context of the sentences are being studies or extracted from the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6ae74f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data processed: \n",
      "0        Job interviews always make sweat bullets makes...\n",
      "1                                         nervous prepared\n",
      "2        feel like getting prepared curve ball thrown t...\n",
      "3                                         Yes stay calm ok\n",
      "4                                           hard stay clam\n",
      "                               ...                        \n",
      "11346    Yeah understand Sometimes position enjoy busy ...\n",
      "11347    past spring bad storm tornado touch neighbors ...\n",
      "11348                                  Oh gosh super scary\n",
      "11349    sounded like train going house basement Thankf...\n",
      "11350         Yeah thank god safe know would done happened\n",
      "Name: utterance, Length: 11351, dtype: object\n",
      "------------------------------------------------------\n",
      "Test Data processed: \n",
      "0       sad read article newborn girl died parents bel...\n",
      "1                           Ugh articles always get wrong\n",
      "2       born premature home hard time breathing instea...\n",
      "3                             Jeez unfortunate sad really\n",
      "4       yes believe God prayers goodness gracious plea...\n",
      "                              ...                        \n",
      "1372    Hahaha kind goofy way Yeah sometimes nothing g...\n",
      "1373                    Christmas best time year back day\n",
      "1374                       still think always get excited\n",
      "1375                              Yeah like nothing worry\n",
      "1376                       yep thats one things make nice\n",
      "Name: utterance, Length: 1377, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "# Getting the list of stopwords and appending additional words to it\n",
    "stopwords_list = list(set(stopwords.words('english')))\n",
    "\n",
    "# Remove train data stopwords and covert to lower case\n",
    "train_data_stop_removed = train_data_list_cleaned.apply(lambda row: row.replace('_comma_', ''))\n",
    "train_data_stop_removed = train_data_stop_removed.apply(lambda row: [word for word in word_tokenize(row) if not word.lower() in stopwords_list])\n",
    "train_data_stop_removed = train_data_stop_removed.str.join(' ')\n",
    "print(\"Train Data processed: \")\n",
    "print(train_data_stop_removed)\n",
    "\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "# Remove test data stopwords and covert to lower case\n",
    "test_data_stop_removed = test_data_list_cleaned.apply(lambda row: row.replace('_comma_', ''))\n",
    "test_data_stop_removed = test_data_stop_removed.apply(lambda row: [word for word in word_tokenize(row) if not word.lower() in stopwords_list])\n",
    "test_data_stop_removed = test_data_stop_removed.str.join(' ')\n",
    "print(\"Test Data processed: \")\n",
    "print(test_data_stop_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "fc627e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of non-zero cells is: 72450\n",
      "The size of encoding array is: 94939764\n",
      "The percentage of words present is: 0.07631154423345733%\n"
     ]
    }
   ],
   "source": [
    "# Creating the bag of words encoding again  \n",
    "train_count_vectorizer = CountVectorizer()\n",
    "X_stop_removed = train_count_vectorizer.fit_transform(train_data_stop_removed)\n",
    "\n",
    "train_one_hot_encoding = X_stop_removed.toarray()\n",
    "\n",
    "train_one_hot_encoding_present_count = 0\n",
    "for arr in train_one_hot_encoding:\n",
    "    train_one_hot_encoding_arr_present_count = (arr > 0).sum()\n",
    "    train_one_hot_encoding_present_count += train_one_hot_encoding_arr_present_count\n",
    "\n",
    "print(\"The count of non-zero cells is: {0}\".format(train_one_hot_encoding_present_count))\n",
    "print(\"The size of encoding array is: {0}\".format(train_one_hot_encoding.size))\n",
    "print(\"The percentage of words present is: {0}%\".format(train_one_hot_encoding_present_count / train_one_hot_encoding.size * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728dba46",
   "metadata": {},
   "source": [
    "## 4. Normalization using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "5215eb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(11351, 8364)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the training data using TD-IDF transformer\n",
    "\n",
    "train_tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "train_embedding_tfidf_transformer = train_tfidf_transformer.fit_transform(X_stop_removed)\n",
    "train_embedding_tfidf = train_embedding_tfidf_transformer.toarray()\n",
    "\n",
    "print(train_embedding_tfidf)\n",
    "print(train_embedding_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95dd04",
   "metadata": {},
   "source": [
    "## 5. Build a SGD classifier and perfrom error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e8f0f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruome\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\ruome\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1632: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6201888162672476\n",
      "F1 score: 0.6201888162672476\n",
      "Confusion matrix: \n",
      " [[215  27  30  26]\n",
      " [ 31 228  54  42]\n",
      " [ 53  54 229  38]\n",
      " [ 36  69  63 182]]\n",
      "f1 score using SGD classifier is: 0.6206513585802701\n"
     ]
    }
   ],
   "source": [
    "X_train = train_embedding_tfidf_transformer\n",
    "y_train = np.array(train_labels_encoded)\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Using training data vocabulary on test data so that the features are consistent    \n",
    "test_count_vectorizer = CountVectorizer(vocabulary = train_count_vectorizer.get_feature_names())\n",
    "X_test = test_count_vectorizer.fit_transform(test_data_stop_removed)\n",
    "\n",
    "test_one_hot_encoding = X_test.toarray()\n",
    "\n",
    "for arr in test_one_hot_encoding:\n",
    "    arr[arr > 0] = 1\n",
    "\n",
    "# Normalizing the test data  \n",
    "test_tfidf_transformer = TfidfTransformer(smooth_idf=False, use_idf=True)\n",
    "test_embedding_tfidf_transformer = test_tfidf_transformer.fit_transform(test_one_hot_encoding)\n",
    "\n",
    "# Getting predictions on test data\n",
    "test_predicted_labels = clf.predict(test_embedding_tfidf_transformer)\n",
    "\n",
    "\n",
    "# do some evaluation on the test set\n",
    "print('Test accuracy:', np.mean(test_labels_encoded == test_predicted_labels))\n",
    "\n",
    "f1_score_vector = f1_score(test_labels_encoded, test_predicted_labels, average=None)\n",
    "\n",
    "print('F1 score:', np.mean(test_labels_encoded == test_predicted_labels))\n",
    "\n",
    "print('Confusion matrix: \\n', confusion_matrix(test_labels_encoded, test_predicted_labels))\n",
    "\n",
    "print('f1 score using SGD classifier is:', np.mean(f1_score_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ce077e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassfied exhibits: [1, 2, 6, 8]\n",
      "\n",
      "--------------------------------------\n",
      "Misclassified Example 1\n",
      "Index: 1\n",
      "Predicted Sentiment: jealous\n",
      "Actual Sentiment: sad\n",
      "Utterance: Ugh_comma_ those articles always get me too       What was wrong with her  \n",
      "--------------------------------------\n",
      "Misclassified Example 2\n",
      "Index: 2\n",
      "Predicted Sentiment: terrified\n",
      "Actual Sentiment: sad\n",
      "Utterance: she was born premature at home_comma_ she had hard time breathing on her own but instead of taking her to the doctor parents were just praying\n",
      "--------------------------------------\n",
      "Misclassified Example 3\n",
      "Index: 6\n",
      "Predicted Sentiment: sad\n",
      "Actual Sentiment: joyful\n",
      "Utterance: 3 years is a long time  How come \n",
      "--------------------------------------\n",
      "Misclassified Example 4\n",
      "Index: 8\n",
      "Predicted Sentiment: sad\n",
      "Actual Sentiment: joyful\n",
      "Utterance: Oh I see  They must miss you_comma_ too \n"
     ]
    }
   ],
   "source": [
    "# Misclassified examples\n",
    "misclassified_exhibits = list(np.where(test_labels_encoded != test_predicted_labels)[0])[:4]\n",
    "print(\"Misclassfied exhibits: {0}\\n\".format(misclassified_exhibits))\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Misclassified Example 1\")\n",
    "index_1 = misclassified_exhibits[0]\n",
    "print(\"Index:\", index_1)\n",
    "print(\"Predicted Sentiment:\", train_labels_unique[test_predicted_labels[index_1]])\n",
    "print(\"Actual Sentiment:\", test_data['context'].iloc[index_1])\n",
    "print(\"Utterance:\", test_data['utterance'].iloc[index_1])\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Misclassified Example 2\")\n",
    "index_2 = misclassified_exhibits[1]\n",
    "print(\"Index:\", index_2)\n",
    "print(\"Predicted Sentiment:\", train_labels_unique[test_predicted_labels[index_2]])\n",
    "print(\"Actual Sentiment:\", test_data['context'].iloc[index_2])\n",
    "print(\"Utterance:\", test_data['utterance'].iloc[index_2])\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Misclassified Example 3\")\n",
    "index_3 = misclassified_exhibits[2]\n",
    "print(\"Index:\", index_3)\n",
    "print(\"Predicted Sentiment:\", train_labels_unique[test_predicted_labels[index_3]])\n",
    "print(\"Actual Sentiment:\", test_data['context'].iloc[index_3])\n",
    "print(\"Utterance:\", test_data['utterance'].iloc[index_3])\n",
    "\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Misclassified Example 4\")\n",
    "index_4 = misclassified_exhibits[3]\n",
    "print(\"Index:\", index_4)\n",
    "print(\"Predicted Sentiment:\", train_labels_unique[test_predicted_labels[index_4]])\n",
    "print(\"Actual Sentiment:\", test_data['context'].iloc[index_4])\n",
    "print(\"Utterance:\", test_data['utterance'].iloc[index_4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcb14d",
   "metadata": {},
   "source": [
    "##### My thoughts\n",
    "Among these misclassification examples, example 1 and 2 are the replies to the same prompt. This prompt is about a sad incident that a newborn babygirl died due to being premature and her parents refused to take her to the hospital. The two utterances are both expressing sadness for the babygirl's misfortune. \n",
    "\n",
    "However, the first example was predicted to be \"jealous\" and the second was \"terrified\". In my view, the cause might be that the first example's \"ugh\" actually have a mixed feeling of \"anger\" towards the irresponsible parents, and \"what was wrong with her\" in everyday conversations do have some negative sentiments. The second example's \"premature\", \"hard time breathing\", \"praying\" could be misleading to the classifier because they do convey a sentiment of a terrified state. It is also a mixed sentiment.\n",
    "\n",
    "Misclassification example 3 and 4 are also replies to the same prompt. This prompt is about someone announcing that they are going to visit their parents soon. The two examples are both expressing joy towards this great news.\n",
    "However, example 3 and 4 are both predicted to be \"sad\". My guess is that the words/phrases \"A long time\", \"How come\", \"miss\" do tend to convey negative sentiments in everyday situations, thus, the classifier grouped these as \"sad\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2599078",
   "metadata": {},
   "source": [
    "## 6. Build a classifier using pre-trained word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "7eb99d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the data\n",
    "train_tokens = [nltk.word_tokenize(sentences) for sentences in train_data_stop_removed]\n",
    "train_y = np.array(train_labels_encoded)\n",
    "\n",
    "test_tokens = [nltk.word_tokenize(sentences) for sentences in test_data_stop_removed]\n",
    "test_y = np.array(test_labels_encoded)\n",
    "\n",
    "# Loading the pretrained word2vec model from Google\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "47e3934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0         1         2         3         4         5    \\\n",
      "0           0.152344 -0.121094  0.102051 -0.083984 -0.184570  0.015320   \n",
      "Job         0.206055 -0.053467 -0.318359 -0.265625  0.287109  0.065918   \n",
      "interviews  0.005646  0.114746 -0.177734 -0.162109  0.298828  0.097168   \n",
      "always      0.055908  0.057617  0.015198  0.251953 -0.041260  0.074219   \n",
      "make       -0.113281 -0.036865  0.094238  0.007996  0.024902 -0.166992   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "k           0.040771 -0.178711  0.226562  0.326172  0.441406 -0.175781   \n",
      "Name        0.039307 -0.155273  0.052490 -0.171875 -0.112305 -0.062988   \n",
      "utterance   0.109863 -0.228516  0.095215  0.201172 -0.542969  0.023315   \n",
      "Length     -0.010925 -0.051758 -0.003403  0.287109  0.171875 -0.097168   \n",
      "object      0.292969 -0.062988  0.083496  0.043457 -0.562500  0.154297   \n",
      "\n",
      "                 6         7         8         9    ...       290       291  \\\n",
      "0           0.238281 -0.478516  0.072754  0.218750  ...  0.028320 -0.164062   \n",
      "Job        -0.018433  0.222656  0.128906  0.312500  ...  0.027710 -0.007812   \n",
      "interviews -0.019775 -0.138672  0.186523 -0.146484  ...  0.025391 -0.161133   \n",
      "always      0.118164 -0.071289  0.083496  0.091309  ... -0.211914  0.101074   \n",
      "make        0.036621  0.073242  0.213867 -0.038574  ... -0.127930  0.137695   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "k          -0.040527 -0.155273  0.225586 -0.227539  ... -0.246094  0.173828   \n",
      "Name       -0.328125 -0.099609  0.235352  0.210938  ... -0.041260  0.247070   \n",
      "utterance   0.183594 -0.239258  0.609375  0.179688  ...  0.043701 -0.078613   \n",
      "Length     -0.250000  0.277344  0.005402 -0.326172  ...  0.089355  0.388672   \n",
      "object      0.273438 -0.052979  0.084961  0.145508  ... -0.201172  0.255859   \n",
      "\n",
      "                 292       293       294       295       296       297  \\\n",
      "0          -0.173828  0.361328 -0.201172 -0.142578 -0.021606  0.013794   \n",
      "Job        -0.277344  0.146484  0.182617  0.223633 -0.177734 -0.059570   \n",
      "interviews  0.253906  0.205078 -0.110352 -0.289062  0.161133  0.003815   \n",
      "always     -0.130859  0.018311 -0.129883 -0.088867  0.100586 -0.091797   \n",
      "make       -0.123535 -0.008911 -0.129883 -0.034180  0.041260 -0.048584   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "k           0.056641  0.250000 -0.488281 -0.154297 -0.056641  0.088867   \n",
      "Name       -0.206055 -0.143555  0.101074  0.008179  0.004211 -0.375000   \n",
      "utterance  -0.082520 -0.211914  0.073242 -0.460938 -0.041992  0.033691   \n",
      "Length     -0.221680  0.020020 -0.069824  0.047607 -0.094238  0.074707   \n",
      "object      0.009399 -0.043457 -0.181641 -0.013916  0.040283  0.011292   \n",
      "\n",
      "                 298       299  \n",
      "0          -0.057373  0.277344  \n",
      "Job        -0.195312 -0.099121  \n",
      "interviews -0.129883 -0.216797  \n",
      "always      0.035889 -0.096680  \n",
      "make        0.294922 -0.202148  \n",
      "...              ...       ...  \n",
      "k          -0.115723  0.231445  \n",
      "Name       -0.445312  0.237305  \n",
      "utterance  -0.328125  0.193359  \n",
      "Length     -0.204102 -0.141602  \n",
      "object     -0.143555  0.043457  \n",
      "\n",
      "[70 rows x 300 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
    "vector_list = [model[word] for word in words if word in model]\n",
    "\n",
    "# Create a list of the words corresponding to these vectors\n",
    "words_filtered = [word for word in words if word in model]\n",
    "\n",
    "# Zip the words together with their vector representations\n",
    "word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "# Cast to a dict so we can turn it into a DataFrame\n",
    "word_vec_dict = dict(word_vec_zip)\n",
    "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "eec0bd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruome\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
    "\n",
    "tsne_df = tsne.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16e236",
   "metadata": {},
   "source": [
    "## 7. Build a classifier based on BERT and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "871b29a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "# load the tokenizer and the model of distilbert-base-uncased\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def sentenceVec(doc):\n",
    "    sentenceVec.i = sentenceVec.i + 1\n",
    "    \n",
    "    if (sentenceVec.i % 1000 == 0):\n",
    "        print(\"Classifier Trainining Iterations: {0}\".format(sentenceVec.i))\n",
    "        \n",
    "    inputs = tokenizer(doc, return_tensors=\"pt\", truncation = True)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    sentence_embedding = torch.mean(last_hidden_states[0], dim=0).detach().numpy()\n",
    "    \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "2db189fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classifier Trainining Iterations: 1000\n",
      "BERT Classifier Trainining Iterations: 2000\n",
      "BERT Classifier Trainining Iterations: 3000\n",
      "BERT Classifier Trainining Iterations: 4000\n",
      "BERT Classifier Trainining Iterations: 5000\n",
      "BERT Classifier Trainining Iterations: 6000\n",
      "BERT Classifier Trainining Iterations: 7000\n",
      "BERT Classifier Trainining Iterations: 8000\n",
      "BERT Classifier Trainining Iterations: 9000\n",
      "BERT Classifier Trainining Iterations: 10000\n",
      "BERT Classifier Trainining Iterations: 11000\n",
      "BERT Classifier Trainining Iterations: 12000\n",
      "BERT Classifier Trainining Iterations: 13000\n",
      "BERT Classifier Trainining Iterations: 14000\n",
      "BERT Classifier Trainining Iterations: 15000\n",
      "BERT Classifier Trainining Iterations: 16000\n",
      "BERT Classifier Trainining Iterations: 17000\n",
      "BERT Classifier Trainining Iterations: 18000\n",
      "BERT Classifier Trainining Iterations: 19000\n",
      "BERT Classifier Trainining Iterations: 20000\n",
      "BERT Classifier Trainining Iterations: 21000\n",
      "BERT Classifier Trainining Iterations: 22000\n"
     ]
    }
   ],
   "source": [
    "# Transform raw train data\n",
    "sentenceVec.i = 0;\n",
    "train_data_raw = pd.DataFrame(train_data)\n",
    "train_sentence_vectors =  train_data_raw.applymap(lambda doc: sentenceVec(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "75cee7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=150, max_iter=300, random_state=42)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainig MLP Classifier\n",
    "mlp_X_train = pd.DataFrame(train_sentence_vectors['utterance'].tolist()).to_numpy()\n",
    "mlp = MLPClassifier(hidden_layer_sizes=150, random_state = 42, max_iter=300)\n",
    "mlp.fit(mlp_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "83fa13b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Trainining Iterations: 1000\n",
      "Classifier Trainining Iterations: 2000\n"
     ]
    }
   ],
   "source": [
    "# Transform raw test data\n",
    "sentenceVec.i = 0;\n",
    "test_data_raw = pd.DataFrame(test_data)\n",
    "test_sentence_vectors =  test_data_raw.applymap(lambda doc: sentenceVec(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "9f37a22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6122004357298475\n",
      "f1-score: 0.6109740351403672\n",
      "Confusion Matrix:\n",
      " [[213  32  38  15]\n",
      " [ 40 219  46  50]\n",
      " [ 54  54 218  48]\n",
      " [ 29  67  61 193]]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "X_test = pd.DataFrame(test_sentence_vectors['utterance'].tolist()).to_numpy()\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(test_labels_encoded, y_pred)\n",
    "f1 = metrics.f1_score(test_labels_encoded, y_pred, average=\"weighted\")\n",
    "cf_matrix = confusion_matrix(test_labels_encoded, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"f1-score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892e88a",
   "metadata": {},
   "source": [
    "## 8. Read the paper at https://arxiv.org/pdf/1811.00207.pdf  and answer the following questions: \n",
    "###### 1)  What does this paper mean by \"fine-tuning\" results? How might you use such fine-tuning in building an empathetic chatbot? \n",
    "The “fine-tuning” results in this paper means the results produced from the classifier based on ED utterance data. Using such fine-tuning techniques, we can improve human metrics on the ED task, in both retrieval and generative set-ups, and then better analyze how to generate more empathetic dialogues when developing the chatbot. \n",
    "###### 2)  What properties of the transformer architecture make it well suited for this application? \n",
    "The transformer includes five layers instead of four, indicating that it has a greater potential for learning and can recognize more complex representations of the input data. Additionally, this transformer features a BERT-based architecture that can classify the next sentence, greatly increasing its accuracy. Additionally, this transformer has both retrieval-based and generative setups. The generative set-up creates a prediction of a likely sentence, and the retrieval-based set-up helps in locating the utterance with the highest likelihood from the batch. The combination enhances and provides the best possible conversations.\n",
    "###### 3)  Explain the metrics used to evaluate performance in Table 1 (P@1,100, AVG-BLEU, and PPL).   \n",
    "P@1,100 is the precision of retrieving the correct test candidate out of 100 test candidates. It reflects how well the model chose the correct response from 100 randomly chosen samples in the test set. Contrary to inference from the retrieval systems for all other metrics, which only uses training utterances as candidates, when we compute p@1,100, the actual response is included in the candidates.\n",
    "\n",
    "AVG-BLEU is the average of BLEU-1,-2,-3,-4. The BLEU score is a metric used to assess how similarly the text that was machine translated compared to a group of high quality reference translations. It compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is.\n",
    "\n",
    "PPL is the metric of perplexity. It is one of the most common metrics for evaluating language models. Perplexity is used to measure sensibility of responses and how well the responses correlate with human evaluations. The lower this metric is, the more easily people are able to understand the sentence.\n",
    "###### 4)  Which of the metrics do you think provides the best measure of performance of empathic systems and why?   \n",
    "When evaluating the performance of empathic systems, AVG-BLEU provides the best measure. P@1,100 measures the precision of retrieving the correct test candidate. PPL measures how well people can understand the sentence. Only AVG-BLEU measures how close the machine-produced response resembles the human response, which ensures the sentiment of the response.\n",
    "###### 5)  Based on Tables 1 and 2, and your reading of the paper, what do you think would help the system get to human-level performance? \n",
    "Based on the paper, providing retrieval candidates of the dataset and fine-tuning conversation models lead to responses that are closer to human-level performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec8f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
